---
title: "ISLR Exercises Chapter 4"
author: "Yigit Ozan Berk"
date: "10/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
# 4.6 Lab

Smarket data from ISLR library
```{r}
library(ISLR)
data = Smarket
```

Time series data.

Percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005.

For each date, the percentage returns for each of the five previous trading days are recorded.(daily)

Also recorded variables:
Volume : the number of shares traded on the previous day, in billions.
Today : the percentage return on the date in question
Direction : whether the market was Up or Down on this date.


```{r}
str(data)
```

```{r}
cor(data[, -9])
```

Correlation between the lag variables and today's returns are close to zero. In other words, there appears to be little correlation between today's returns and previous days' returns. the only substantial correlation is between Year and Volume. 

```{r}
plot(Smarket$Volume)
```

volume is increasing over time. The average number os shares traded daily increased from 2001 to 2005.

## Logistic Regression

logistic regression model to predict Direction using lag1 through lag5 and volume. 
```{r}
glm.fits = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial)
summary(glm.fits)
```

smallest p-value is associated with lag1. however, at a value of 0.15, there is no clear evidence of a real association between lag1 and direction.

```{r}
coef(glm.fits)
```

```{r}
summary(glm.fits)$coef
```

using the predict function:
type = "response" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. 

If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model.



```{r}
glm.probs = predict(glm.fits, type = "response")
glm.probs[1:10]
```

These values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.

```{r}
contrasts(Direction)
```

In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.

```{r}
glm.pred = rep("Down", 1250)
#creates a vector of 1250 Down elements
glm.pred[glm.probs > .5] = "Up"
#transforms to Up all of the elements for which the predicted probability of a market increase exceeds 0.5
```

```{r}
table(glm.pred, Direction)
```

```{r}
(507 + 145) / 1250
mean(glm.pred == Direction)
```

At first glance, it appears that logistic regression model is working a little better than random guessing. But,
This is the *training* error rate. 100 - 52.2 == 47.8 %

we can fit the model using part of the data, and then examine how well it predicts the held out data.

```{r}
train= (Year < 2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
Direction.2005 = Direction[!train]
```

fit a logistic regression model using only the subset of the observations that correspond to the dates before 2005, using the *subset* argument
```{r}
glm.fits1 = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial, subset = train)
glm.probs = predict(glm.fits1 , Smarket.2005, type = "response")
```

```{r}
glm.pred = rep("Down", 252)
#Smarket.2005 has 252 observations
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```

The test error rate is 52%, which is worse than random guessing!

Perhaps removing the variables that appear not to be helpful(low p-values) can make the model more effective.

Below we have refit the logistice regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model

```{r}
glm.fits2 = glm(Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train)
glm.probs = predict(glm.fits2, Smarket.2005, type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)

```

```{r}
106/(106 + 76)
```

## LDA Analysis

in R, we fit an LDA model using the lda() function, which is part of the MASS library.

```{r}
library(MASS)
lda.fit = lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda.fit
```



```{r}
plot(lda.fit)
```

LDA output indicates that x_prior1 = 0.492 and x_prior2 = 0.508; in other words, 49.2% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of mu_k. These suggest that there is a tendency for the previous 2 days' returns to be negative on days when the market increases, and a tendency for the previous days' returns to be positive on days when the market declines. 

Plot function produces plots of the linear discriminants, obtained by computing -.642 * Lag1 - .514 * Lag2 for each of the training observations.

predict() function returns a list with three elements. the first element, class, contains LDA's predictions about the movement of the market. the second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class. finally, x contains the linear discriminants.
```{r}
lda.pred = predict(lda.fit, Smarket.2005)
names(lda.pred)
```

LDA and logistic regression predictions are almost identical here

```{r}
lda.class = lda.pred$class
table(lda.class, Direction.2005)
```

```{r}
mean(lda.class == Direction.2005)
```

applying a 50% threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class

```{r}
sum(lda.pred$posterior[,1]>= .5)
sum(lda.pred$posterior[,1] <.5)
```

Notice that the posterior probability output by the model corresponds to the prob that the market will decrease:
```{r}
lda.pred$posterior[1:20, 1]
lda.class[1:20]
```

if we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. for instance, supppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day - say, if the posterior probability is at least 90%.
```{r}
sum(lda.pred$posterior[ ,1] >.9)
```

No days in 2005 meet that threshold! In fact, the greateest posterior prob of decrease in all of 2005 was 52.02%.

## QDA

```{r}
qda.fit = qda(Direction ~ Lag1 + Lag2, data = Smarket, subset= train)
qda.fit
```

The output contains the group means, But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than linear, function of the predictors. 


```{r}
qda.class = predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
```

Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. However, we recommend evaluating this method's performance on a larger test set before betting that this approach will consistently beat the market!

## KNN

This function works rather differently from the other model-fitting functions. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command. the function requires four inputs.
1. a matrix containing the predictors associated with the training data, labeled train.X below.
2. a matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below.
3. a vector containing the class labels for the training observations, labeled train.Direction below.
4. a value for K, the number of nearest neighbors to be used by the classifier.

```{r}
library(class)
train.X = cbind(Smarket$Lag1, Smarket$Lag2)[train, ]
test.X = cbind(Smarket$Lag1, Smarket$Lag2)[!train, ]
train.Direction = Smarket$Direction[train]
```

Now the knn() function can be used to predict the market's movement for the dates in 2005.
```{r}
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2005)
```

```{r}
(83+43)/252
```
Not very good results
K = 3

```{r}
knn.pred = knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```

The results have improved slightly, but increasing K further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.

## Caravan insurance data application

This data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.


```{r}
library(ISLR)
dim(Caravan)
str(Caravan)
```

Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scalse will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, the nwe'd get quite different classification results from what we get if these two variables are measured in dollars and years.

A good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one.
 the scale() function does that.. we exlude column 86, because that is the qualitative Purchase variable.
 
```{r}
standardized.X = scale(Caravan[, -86])
var(Caravan[, 1])
var(standardized.X[, 1])
```
 
 Now every column of standardized.X has a standard deviation of 1 and a mean of 0.
